import sys
import os
import shutil
import datetime
import numpy as np
'''
Usage: python 2_haplo_summary.py [full path to 'batch_#.haplotypes.tsv'] [path to a directory to create the output dir in]

This script does a lot of summarizing, and uses the batch_#.haplotypes.tsv output from the 
populations module of Stacks as input. An output directory called 'Summary_Output_A'
or 'Summary_Output_B' is created in a user selected directory and several output files
are created here. The naming of output directory will depend on the user choice below.

First a decision will need to be made concerning loci that contain at least one sample with
more than two haplotypes (indicating "polyploidy", sequencing error, or possibly paralogy).
Those individuals can 1) be re-coded as missing data (-), or 2) the whole locus can be removed.
In some cases, it may be a single sample with this condition, in other loci it may be
a majority of samples. Removing entire loci could drop the number by a substantial amount,
whereas coding as missing could substantially drive up the missing data level for the locus.
You can run the summary both ways, as two different directories will be made, and compare.
My practical experience indicates these potentially paralogous loci should be completely
eliminated, and a conservative approach should be taken. 

The following steps will be completed:

1) Number of loci counted. [function: loci_number]

2) Number of loci with no variation ("consensus") are counted and moved to a separate
file 'batch_#.identical_haplotypes.tsv'. [function: identical_loci]

3) Number of loci with too many haplotypes per individual (>2) are identified, counted, 
and moved to a separate file 'batch_#.poly_haplotypes.tsv'. [function: poly_haplos]

4) The number of SNP sites in each locus is counted and a tab-delimited output file is 
created with "Sites" and "Catalog_ID" headers, called 'Variable_Sites_Freq_Distribution.txt'.
This can be visualized to examine the frequency of lengths of variable sites across loci.
[function: haplo_length]

5) For each individual, the level of missing data is assessed across all loci.  This 
information is written to a tab-delimited text file, 'Missing_Data_per_Sample.txt', 
with the headers "Sample", "Loci_With_Missing_Data", and "Perc_Missing_Data". This is 
useful for identifying low quality samples that may be removed. [function: missing_data]

6) For each locus, each SNP site is examined to determine if it is biallelic or not. If a
site is identified, the locus and SNP site (starting from position '0') are recorded in the 
main log file. These loci are excluded from the final output. This could be refined to
avoid a particular SNP site within a locus in the future. [function: biallelic]

7) For analysis (a), the loci flagged in steps 2, 3, and 6 are tabulated and excluded from 
the following steps. For (b), the loci flagged in steps 2 and 6 only are tabulated and 
excluded from the following steps. The flagged loci are recorded in output file 
'Loci_Excluded.txt'. [function: list_gathering] or [function: list_gathering_variation]

8) Main output 1: A new tsv file is generated by selecting the FIRST SNP site within a locus
if the locus has more than one SNP site. This only includes loci passing filters above.
Output is written as 'batch_1.singleFirstSNP_haplotypes.tsv' and matches format of original
'batch_#.haplotypes.tsv' input file. For (b) polyhaplotypes are coded as missing data, 
rather than a SNP site, and an output file "Polyhaplotypes_to_Missing_FirstSNP.txt" is 
made which tells how many samples were re-coded as missing data for each locus as a result.
[function: output_tsv] or [function: output_tsv_variation]

9) Main output 2: A new tsv file is generated by selecting a RANDOM SNP site within a locus
if the locus has more than one SNP site. This only includes loci passing filters above.
Output is written as 'batch_1.singleRandomSNP_haplotypes.tsv' and matches format of original
'batch_#.haplotypes.tsv' input file. The random SNP site chosen is recorded in an output 
file called 'Random_SNPs.txt' so the sites can actually be tracked back to original input
tsv file, if desired. This file has information for all loci, even if there is only one SNP
site, and in this case the position will always be [0]. For (b) polyhaplotypes are coded 
as missing data, rather than a SNP site, , and an output file 
"Polyhaplotypes_to_Missing_FirstSNP.txt" is made which tells how many samples were 
re-coded as missing data for each locus as a result.
[function: output_tsv2] or [function: output_tsv2_variation]

A log file is written with much of the information printed to terminal window, called 
'Summary.log'.

############################################
Written for Python 2.7.3
External Dependencies: Numpy (Numerical Python)
############################################

Dan Portik
daniel.portik@uta.edu
February 2016
'''

print '\n',"##############################################################################"
print "In the case of a locus with at least one sample with more than two haplotypes," 
print "do you want to:"
print '\t', "a. Remove the entire locus from the final output."
print '\t', "b. Code all those samples as missing data (-), and keep the locus (not recommended because of paralogy issues!)."
print "##############################################################################",'\n'

decision_analysis = (raw_input("Please select one of the above options (a or b), or 'q' to quit: "))
print '\n'

#get haplotype file path
hap_file = sys.argv[1]
fh_hap = open(hap_file, 'r')

#break up path name to get appropriate parts of haplotype file name
file_name = hap_file.split('/')
file_name = file_name[-1]
file_names = file_name.split('.')
batch = file_names[0]
haplo_name = file_names[1]

#load file lines into memory for downstream functions
hap_lines = fh_hap.readlines()

#=================================================================================================
#simple calculation function
#create function to convert lists into arrays and do very basic stats
def quickstats(x):
    x_array = np.asarray(x,dtype=np.float64)
    x_avg = np.average(x_array)
    x_avg = np.around(x_avg, decimals = 1)
    x_min = np.amin(x_array)
    x_max = np.amax(x_array)
    x_median = np.median(x_array)
    return x_avg, x_min, x_max, x_median
#=================================================================================================
#simple function to calculate percents
def percent_calc(x,y):
    percentage = float( ( float(x) / float(y)) * float(100))
    percentage = np.around(percentage, decimals = 1)
    return percentage

#=================================================================================================
#function for simply counting number of loci
def loci_number(lines):
    #begin a counter and add one for every line past the header
    loci_count = int(0)
    for line in lines[1:]:
        loci_count+=1
        
    print '\n', '\n', "Number of loci in {0} = {1}.".format(file_name, loci_count)
    fh_sum.write("Number of loci in {0} = {1}.".format(file_name, loci_count)+'\n')
    return loci_count

#=================================================================================================
#function for writing a new haplo file for loci with no variation ("consensus" loci)
def identical_loci(lines):
    print '\n', "========================================================"
    print "Now looking for loci with no variation..."
    ident_count = int(0)

    invariant_list = []
    
    ident_output = batch+".identical_haplotypes.tsv"
    fh_ident = open(ident_output, 'a')

    header = lines[0].strip()
    fh_ident.write(header+'\n')
    
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        #skip first two columns ('catalog ID' and 'count'), search if haplo is called "consensus", indicating no variation
        if 'consensus' in line_list[2:]:
            fh_ident.write(line+'\n')
            ident_count+=1
            print '\t', "Locus {} has no variation".format(line_list[0])
            invariant_list.append(line_list[0])
                
    print "Number of loci with no variation = {}.".format(ident_count)
    print '\n',"{0} file is written to {1} directory.".format(ident_output,out_dir)
    fh_sum.write("Number of loci with no variation = {}.".format(ident_count)+'\n')
    fh_sum.write("'{0}' file is written to {1} directory.".format(ident_output,out_dir)+'\n'+'\n')
    fh_ident.close()
    return invariant_list

#=================================================================================================
#function for writing new haplo file for loci showing more than two haplos for an individual
def poly_haplos(lines):
    print '\n', "========================================================"
    print "Now looking for loci with more than two haplotypes per individual..." 

    poly_output = batch+".poly_haplotypes.tsv"
    fh_poly = open(poly_output, 'a')

    multiple_haplos_list = []
    poly_count = int(0) 

    header = lines[0].strip()
    fh_poly.write(header+'\n')
    
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        #create flag for lines with multiple haplos per ind
        bad_flag = int(0)
        #skip first two columns ('catalog ID' and 'count')
        for haplo in line_list[2:]:
            #look for individuals with multiple haploytpes (ex. AA/GA)
            if '/' in haplo:
                haplo_no = haplo.split('/')
                #find how many haplotypes are here (length of list <=2 is okay)
                if len(haplo_no) >= int(3):
                    #count individuals with too many haplos
                    bad_flag+=1
        #normal loci should have bad_flag score of 0, but if they don't:
        if bad_flag >= int(1):
            #write line to output, print how many ind have multiple haplos
            fh_poly.write(line+'\n')
            poly_count+=1
            print '\t', "Locus {0} contains {1} individuals with more than 2 haplotypes".format(line_list[0],bad_flag)
            multiple_haplos_list.append(line_list[0])

    #Summarize number of loci not passing this filter
    print "Number of loci containing at least one individual with more than 2 haplotypes = {}.".format(poly_count)
    print '\n',"{0} file is written to {1} directory.".format(poly_output,out_dir)
    fh_sum.write("Number of loci containing at least one individual with more than 2 haplotypes = {}.".format(poly_count)+'\n')
    fh_sum.write("'{0}' file is written to {1} directory.".format(poly_output,out_dir)+'\n'+'\n')
    fh_poly.close()
    return multiple_haplos_list

#=================================================================================================
#function for writing output file with number of variable sites within each locus
def haplo_length(lines):
    print '\n',"========================================================"
    print "Now summarizing number of variable site within each locus..." 

    freq_out = "Variable_Sites_Freq_Distribution.txt"
    fh_freq = open(freq_out, 'a')
    fh_freq.write("Sites"+'\t'+"Catalog_ID"+'\n')

    haplo_lines = []
    length_list = []

    #remove "consensus" loci from search by making new list of filtered lines
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        if "consensus" not in line_list:
            haplo_lines.append(line)

    #Begin looping through each line of true data
    for haplo_line in haplo_lines:
        haplo_line_list = haplo_line.split('\t')
        catalog_ID = haplo_line_list[0]

        #create list of base positions for this locus
        for haplotype in haplo_line_list[2:]:
            #avoid missing data and multiple haplotypes
            if haplotype != '-' and '/' not in haplotype:
                bases_list = []
                for base in haplotype:
                    bases_list.append(base)
        #last entry in reset bases_list will have each base position, count length
        locus_length = len(bases_list)
        #append to length list
        length_list.append(locus_length)
        #write length and catalogID to output file
        fh_freq.write("{}".format(locus_length)+'\t'+catalog_ID+'\n')

    #use quickstats function to turn list to array for stats
    l_avg, l_min, l_max, l_median = quickstats(length_list)
    print '\t', "Average number of variable sites per locus = {}.".format(l_avg)
    print '\t', "Maximum number of variable sites per locus = {}.".format(l_max)
    print '\t', "Median number of variable sites per locus = {}.".format(l_median)
    print '\n', "{0} file is written to {1} directory.".format(freq_out,out_dir)
    fh_sum.write("Average number of variable sites per locus = {}.".format(l_avg)+'\n')
    fh_sum.write("Maximum number of variable sites per locus = {}.".format(l_max)+'\n')
    fh_sum.write("Median number of variable sites per locus = {}.".format(l_median)+'\n')
    fh_sum.write("'{0}' file is written to {1} directory.".format(freq_out,out_dir)+'\n'+'\n')
    fh_freq.close()

#=================================================================================================
#function for examining missing data across individuals
def missing_data(lines):
    print '\n', "========================================================"
    print "Now summarizing missing data for each sample..."

    miss_output = "Missing_Data_per_Sample.txt"
    fh_miss = open(miss_output, 'a')
    fh_miss.write("Sample"+'\t'+"Loci_With_Missing_Data"+'\t'+"Perc_Missing_Data"+'\n')

    perc_list = []
    
    #get number of columns to search through appropriate indices per line list slice
    columns = lines[1].strip()
    columns = columns.split('\t')
    for i in range(len(columns)):
        #skip first two indices (catalogID and counts)
        if i < 2:
            pass
        elif i >= 2:
            missing_count = int(0)
            header = lines[0].strip()
            header = header.split('\t')
            locus_count = int(0)
            for line in lines[1:]:
                #count each locus for total
                locus_count+=1
                line = line.strip()
                line_list = line.split('\t')
                #look for the missing data and keep count
                if line_list[i] == '-':
                    missing_count+=1

            #calculate percentage missing data using function above
            percent = percent_calc(missing_count, locus_count)
            perc_list.append(percent)

            #make not of data with >50% missing data on screen, write all info to screen
            if percent > float(50):
                print '\n', '\t', "*** {0} is missing data for {1} loci ({2}% missing data)".format(header[i],missing_count,percent),'\n'
            else:
                print '\t', "{0} is missing data for {1} loci ({2}% missing data)".format(header[i],missing_count,percent)
            fh_miss.write("{}".format(header[i])+'\t'+"{}".format(missing_count)+'\t'+"{}".format(percent)+'\n')

    #more stats using predefined function above
    p_avg, p_min, p_max, p_median = quickstats(perc_list)
    print '\n', "Average percent missing data across individuals = {}%.".format(p_avg)
    print "Maximum percent missing data across individuals = {}%.".format(p_max)
    print "Median percent missing data across individuals = {}%.".format(p_median)

    fh_sum.write("Average percent missing data across individuals = {}.".format(p_avg)+'\n')
    fh_sum.write("Maximum percent missing data across individuals = {}.".format(p_max)+'\n')
    fh_sum.write("Median percent missing data across individuals = {}.".format(p_median)+'\n')
    print '\n',"{0} file is written to {1} directory.".format(miss_output,out_dir)
    fh_sum.write("'{0}' file is written to {1} directory.".format(miss_output,out_dir)+'\n'+'\n')
    fh_miss.close()

#=================================================================================================
#function for examining whether each site of a locus is biallelic
def biallelic(lines):
    print '\n', "========================================================"
    print "Now determining if all SNP sites are biallelic..."

    haplo_lines = []
    flagged_list = []
    
    biallelic_loci = int(0)
    flagged_loci = int(0)
    #remove "consensus" loci from search
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        if "consensus" not in line_list:
            haplo_lines.append(line)

    #Begin looping through each line of true data
    for haplo_line in haplo_lines:
        haplo_line_list = haplo_line.split('\t')
        catalog_ID = haplo_line_list[0]
        
        if int(haplo_line_list[1]) >= int(2):
            data_present = int(0) 
            #create list of base positions for this locus
            for haplotype in haplo_line_list[2:]:
                if haplotype != '-' and '/' not in haplotype:
                    bases_list = []
                    for base in haplotype:
                        bases_list.append(base)
                        data_present += 1
            ##print "Locus {} has {} positions.".format(catalog_ID, len(bases_list))

            flagged = int(0)
            if data_present >= 1:
                #use the length of the locus to determine the number of indices to search
                for i in range(len(bases_list)):
                    #use set properties (only includes unique values, no duplicates) to list bases per site
                    bases_set = set()
                    #run through each sample haplotype
                    for haplotype in haplo_line_list[2:]:
                        #skip gaps (missing data)
                        if haplotype != '-':
                            #deal with multiple haplotype issue
                            if '/' in haplotype:
                                #break into list of haplotypes
                                subhaplos_list = haplotype.split('/')
                                #take appropriate index slice of haplotype string
                                for subhap in subhaplos_list:                                
                                        bases_set.add(subhap[i])
                                        ##print "position {} = {}".format(i,subhap[i])
                            #otherwise just take slice from the normal haplotype
                            else:
                                for base in haplotype:
                                    bases_set.add(haplotype[i])
                                    ##print "position {} = {}".format(i,haplotype[i])

                    ##print "Base pair types at position {} =".format(i), bases_set

                    #calculate length of set to see if more than two bases are present
                    if len(bases_set) >= int(3):
                        print '\t', "Locus {} SNP position {} is not biallelic!".format(catalog_ID,i)
                        flagged += 1
                        flagged_loci+=1
                        flagged_list.append(catalog_ID)
                        fh_sum.write("Locus {} SNP position {} is not biallelic!".format(catalog_ID,i)+'\n')
            
        if flagged == int(0):
                #print '\t', "Locus {} is biallelic across all SNP positions.".format(catalog_ID)
                biallelic_loci+=1

    print "Total number of biallelic loci = {}.".format(biallelic_loci)
    print "Total number of non-biallelic loci = {}.".format(flagged_loci)
    return flagged_list

#=================================================================================================
#function for gathering catalog_ids to avoid from all previous steps 
def list_gathering(x,y,z,i):
    print '\n', "========================================================"
    print "Calculating number of flagged loci..."
    avoid_set = set()
    for locus in x:
        avoid_set.add(locus)
    for locus in y:
        avoid_set.add(locus)
    for locus in z:
        avoid_set.add(locus)
    avoid_list = list(avoid_set)
    avoid_list.sort(key = int)

    good_loci = int(i) - len(avoid_list)
    
    percent = percent_calc((len(avoid_list)),i)

    remove_output = "Loci_Excluded.txt"
    fh_remove = open(remove_output, 'a')
    for item in avoid_list:
        fh_remove.write(item+'\n')
    fh_remove.close()

    print '\t', "Avoiding {0} of {1} loci based on filtering ({2}%):".format(len(avoid_list),i,percent), '\n'
    print '\t', "{} invariant loci.".format(len(x))
    print '\t', "{} loci with at least one individual with >2 haplotypes.".format(len(y))
    print '\t', "{} loci with at least one non-biallelic SNP site.".format(len(z)), '\n'
    print '\t', "Will create new output 'haplotype.tsv' files from {} loci.".format(good_loci)
    fh_sum.write('\n'+"Avoiding {0} of {1} loci based on filtering ({2}%).".format(len(avoid_list),i,percent)+'\n')
    fh_sum.write("{} invariant loci.".format(len(x))+'\n')
    fh_sum.write("{} loci with at least one individual with >2 haplotypes.".format(len(y))+'\n')
    fh_sum.write("{} loci with at least one non-biallelic SNP site.".format(len(z))+'\n')
    fh_sum.write("Will create new output 'haplotype.tsv' files from {} loci.".format(good_loci)+'\n')

    return avoid_list

#=================================================================================================
#FINALLY a function for writing the main output file, based on FIRST SNP per locus if bp >1
def output_tsv(lines,avoid_list):
    print '\n', "========================================================"
    print "Now writing final output tsv file based on first SNP per locus...", '\n'

    tsv_output = batch+".singleFirstSNP_haplotypes.tsv"
    fh_tsv = open(tsv_output, 'a')

    header = lines[0].strip()
    fh_tsv.write(header+'\n')

    #create subset of filtered loci based on lists created from functions above
    filtered_lines = []
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        if line_list[0] not in avoid_list:
            filtered_lines.append(line)

    #now go through all loci to take first SNP and write to output file
    for line in filtered_lines:
        line = line.strip()
        line = line.split('\t')
        #always write catID and count unchanged on each line
        fh_tsv.write(line[0]+'\t'+line[1]+'\t')
        #iterate through remaining haplotype info
        for haplo in line[2:]:
            #treat gaps (missing data) as same
            if haplo == '-':
                #print "locus = ", haplo[0]
                #print haplo[0]
                fh_tsv.write(haplo[0]+'\t')

            #deal with multiple haplos
            elif '/' in haplo:
                #print "locus = ", haplo
                subhaps = haplo.split('/')
                #test if first SNP across haplos is actually identical
                if subhaps[0][0] == subhaps[1][0]:
                    #if it is, just write one
                    fh_tsv.write("{}".format(subhaps[0][0])+'\t')
                    #print subhaps[0][0]
                #if it isn't the same
                elif subhaps[0][0] != subhaps[1][0]:
                    #write out the heterzygous annotation
                    outhap = subhaps[0][0]+'/'+subhaps[1][0]
                    #print outhap
                    fh_tsv.write(outhap+'\t')

            #And for single bp SNPS, just write as is
            else:
                fh_tsv.write(haplo[0]+'\t')
                #print "locus = ",haplo[0]
                #print haplo[0]
                
        fh_tsv.write('\n')
    fh_tsv.close()
    print "Check output {} in {} directory.".format(tsv_output,out_dir)
    fh_sum.write('\n'+"File '{}' written to {} directory.".format(tsv_output,out_dir)+'\n')
        

#=================================================================================================
#FINALLY a function for writing the main output file, based on RANDOM SNP per locus if bp >1
def output2_tsv(lines,avoid_list):
    print '\n', "========================================================"
    print "Now writing final output tsv file based on RANDOM SNP per locus...", '\n'

    tsv2_output = batch+".singleRandomSNP_haplotypes.tsv"
    fh_tsv2 = open(tsv2_output, 'a')

    rando_output = "Random_SNPs.txt"
    fh_rando = open(rando_output, 'a')
    
    fh_rando.write("Catalog_ID"+'\t'+"SNP_position"+'\n')
        
    header = lines[0].strip()
    fh_tsv2.write(header+'\n')

    #create subset of filtered loci based on lists created from functions above
    filtered_lines = []
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        if line_list[0] not in avoid_list:
            filtered_lines.append(line)

    #now go through all loci to take random SNP and write to output file
    for line in filtered_lines:
        line = line.strip()
        line_list = line.split('\t')
        #always write catID and count unchanged on each line
        fh_tsv2.write(line_list[0]+'\t'+line_list[1]+'\t')

        #iterate through remaining haplotype info
        for haplo in line_list[2:]:
            #avoid missing data and multiple haplotypes
            if haplo != '-':
                if '/' in haplo:
                    subhaplo = haplo.split('/')
                    bases_list = []
                    for base in subhaplo[0]:
                        bases_list.append(base)
                else:
                    bases_list = []
                    for base in haplo:
                        bases_list.append(base)
                    
        #last entry in reset bases_list will have each base position, count length
        locus_length = len(bases_list)
        
        #generate random number from this distribution for index to slice
        index_choice = int(np.random.choice(locus_length, 1))
        #make note of index in output file
        fh_rando.write("{}".format(line_list[0])+'\t'+"{}".format(index_choice)+'\n')

        for haplo in line_list[2:]:
            #treat gaps (missing data) as same
            if haplo == '-':
                ##print haplo[0]
                fh_tsv2.write(haplo[0]+'\t')

            #deal with multiple haplos
            elif '/' in haplo:
                ##print "locus = ", haplo
                subhaps = haplo.split('/')
                #test if first SNP across haplos is actually identical
                if subhaps[0][index_choice] == subhaps[1][index_choice]:
                    #if it is, just write one
                    fh_tsv2.write("{}".format(subhaps[0][index_choice])+'\t')
                    ##print subhaps[0][index_choice]
                #if it isn't the same
                elif subhaps[0][index_choice] != subhaps[1][index_choice]:
                    #write out the heterzygous annotation
                    outhap = subhaps[0][index_choice]+'/'+subhaps[1][index_choice]
                    ##print outhap
                    fh_tsv2.write(outhap+'\t')

            #And for single bp SNPS, just write as is
            else:
                fh_tsv2.write(haplo[index_choice]+'\t')
                ##print haplo[index_choice]
                
        fh_tsv2.write('\n')
    fh_tsv2.close()
    print "Check output {} in {} directory.".format(tsv2_output,out_dir), '\n', '\n'
    fh_sum.write('\n'+"File '{}' written to {} directory.".format(tsv2_output,out_dir)+'\n')


#=================================================================================================
#function for gathering catalog_ids to avoid from all previous steps 
def list_gathering_variation(x,y,i):
    print '\n', "========================================================"
    print "Calculating number of flagged loci..."
    avoid_set = set()
    for locus in x:
        avoid_set.add(locus)
    for locus in y:
        avoid_set.add(locus)
    avoid_list = list(avoid_set)
    avoid_list.sort(key = int)

    good_loci = int(i) - len(avoid_list)
    
    percent = percent_calc((len(avoid_list)),i)

    remove_output = "Loci_Excluded.txt"
    fh_remove = open(remove_output, 'a')
    for item in avoid_list:
        fh_remove.write(item+'\n')
    fh_remove.close()

    print '\t', "Avoiding {0} of {1} loci based on filtering ({2}%):".format(len(avoid_list),i,percent), '\n'
    print '\t', "{} invariant loci.".format(len(x))
    print '\t', "{} loci with at least one non-biallelic SNP site.".format(len(y)), '\n'
    print '\t', "Will create new output 'haplotype.tsv' files from {} loci.".format(good_loci)
    fh_sum.write('\n'+"Avoiding {0} of {1} loci based on filtering ({2}%).".format(len(avoid_list),i,percent)+'\n')
    fh_sum.write("{} invariant loci.".format(len(x))+'\n')
    fh_sum.write("{} loci with at least one non-biallelic SNP site.".format(len(y))+'\n')
    fh_sum.write("Will create new output 'haplotype.tsv' files from {} loci.".format(good_loci)+'\n')

    return avoid_list

#=================================================================================================
#FINALLY a function for writing the main output file, based on FIRST SNP per locus if bp >1
def output_tsv_variation(lines,avoid_list):
    print '\n', "========================================================"
    print "Now writing final output tsv file based on first SNP per locus...", '\n'

    tsv_output = batch+".singleFirstSNP_haplotypes.tsv"
    fh_tsv = open(tsv_output, 'a')

    mia_output = "Polyhaplotypes_to_Missing_FirstSNP.txt"
    fh_mia = open(mia_output, 'a')
    fh_mia.write("Catalog_ID"+'\t'+"Samples_Replaced_With_Missing"+'\n')

    header = lines[0].strip()
    fh_tsv.write(header+'\n')

    #create subset of filtered loci based on lists created from functions above
    filtered_lines = []
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        if line_list[0] not in avoid_list:
            filtered_lines.append(line)

    #now go through all loci to take first SNP and write to output file
    for line in filtered_lines:
        replaced_count = int(0)
        line = line.strip()
        line = line.split('\t')
        #always write catID and count unchanged on each line
        fh_tsv.write(line[0]+'\t'+line[1]+'\t')
        #iterate through remaining haplotype info
        for haplo in line[2:]:
            #treat gaps (missing data) as same
            if haplo == '-':
                #print "locus = ", haplo[0]
                #print haplo[0]
                fh_tsv.write(haplo[0]+'\t')

            #deal with multiple haplos
            elif '/' in haplo:
                #find out if its a polyhaplotype by splitting into list
                multi_haplo_test = haplo.split('/')
                #if there are 2 or less haplotypes here
                if len(multi_haplo_test) <= int(2):
                    #print "locus = ", haplo
                    subhaps = haplo.split('/')
                    #test if first SNP across haplos is actually identical
                    if subhaps[0][0] == subhaps[1][0]:
                        #if it is, just write one
                        fh_tsv.write("{}".format(subhaps[0][0])+'\t')
                        #print subhaps[0][0]
                    #if it isn't the same
                    elif subhaps[0][0] != subhaps[1][0]:
                        #write out the heterzygous annotation
                        outhap = subhaps[0][0]+'/'+subhaps[1][0]
                        #print outhap
                        fh_tsv.write(outhap+'\t')

                #if there are more than 2 haplotypes, replace with gap
                elif len(multi_haplo_test) >= int(3):
                    fh_tsv.write('-'+'\t')
                    replaced_count+=1
                     
            #And for single bp SNPS, just write as is
            else:
                fh_tsv.write(haplo[0]+'\t')
                #print "locus = ",haplo[0]
                #print haplo[0]
        if replaced_count >= int(1):
            fh_mia.write("{}".format(line[0])+'\t'+"{}".format(replaced_count)+'\n')
            print '\t',"Locus {0}: recoded {1} polyhaplotypes".format(line[0],replaced_count)
        if replaced_count == int(line[1]):
            print '\n', '\t', "***WARNING: All samples recoded as missing data for locus {}!!!".format(line[0]), '\n'

        fh_tsv.write('\n')
    fh_tsv.close()
    print "Check output {} in {} directory.".format(tsv_output,out_dir)
    fh_sum.write('\n'+"File '{}' written to {} directory.".format(tsv_output,out_dir)+'\n')
        

#=================================================================================================
#FINALLY a function for writing the main output file, based on RANDOM SNP per locus if bp >1
def output2_tsv_variation(lines,avoid_list):
    print '\n', "========================================================"
    print "Now writing final output tsv file based on RANDOM SNP per locus...", '\n'

    tsv2_output = batch+".singleRandomSNP_haplotypes.tsv"
    fh_tsv2 = open(tsv2_output, 'a')
    
    mia_output2 = "Polyhaplotypes_to_Missing_RandomSNP.txt"
    fh_mia2 = open(mia_output2, 'a')
    fh_mia2.write("Catalog_ID"+'\t'+"Samples_Replaced_With_Missing"+'\n')

    rando_output = "Random_SNPs.txt"
    fh_rando = open(rando_output, 'a')
    
    fh_rando.write("Catalog_ID"+'\t'+"SNP_position"+'\n')
        
    header = lines[0].strip()
    fh_tsv2.write(header+'\n')

    #create subset of filtered loci based on lists created from functions above
    filtered_lines = []
    for line in lines[1:]:
        line = line.strip()
        line_list = line.split('\t')
        if line_list[0] not in avoid_list:
            filtered_lines.append(line)

    #now go through all loci to take first SNP and write to output file
    for line in filtered_lines:
        replaced_count = int(0)
        line = line.strip()
        line_list = line.split('\t')
        #always write catID and count unchanged on each line
        fh_tsv2.write(line_list[0]+'\t'+line_list[1]+'\t')

        #iterate through remaining haplotype info
        for haplo in line_list[2:]:
            #avoid missing data and multiple haplotypes
            if haplo != '-':
                if '/' in haplo:
                    subhaplo = haplo.split('/')
                    bases_list = []
                    for base in subhaplo[0]:
                        bases_list.append(base)
                else:
                    bases_list = []
                    for base in haplo:
                        bases_list.append(base)
                    
        #last entry in reset bases_list will have each base position, count length
        locus_length = len(bases_list)
        
        #generate random number from this distribution for index to slice
        index_choice = int(np.random.choice(locus_length, 1))
        #make note of index in output file
        fh_rando.write("{}".format(line_list[0])+'\t'+"{}".format(index_choice)+'\n')

        for haplo in line_list[2:]:
            #treat gaps (missing data) as same
            if haplo == '-':
                ##print haplo[0]
                fh_tsv2.write(haplo[0]+'\t')

            #deal with multiple haplos
            elif '/' in haplo:
                multi_haplo_test = haplo.split('/')
                if len(multi_haplo_test) <= int(2):
                    ##print "locus = ", haplo
                    subhaps = haplo.split('/')
                    #test if first SNP across haplos is actually identical
                    if subhaps[0][index_choice] == subhaps[1][index_choice]:
                        #if it is, just write one
                        fh_tsv2.write("{}".format(subhaps[0][index_choice])+'\t')
                        ##print subhaps[0][index_choice]
                    #if it isn't the same
                    elif subhaps[0][index_choice] != subhaps[1][index_choice]:
                        #write out the heterzygous annotation
                        outhap = subhaps[0][index_choice]+'/'+subhaps[1][index_choice]
                        ##print outhap
                        fh_tsv2.write(outhap+'\t')
                        
                elif len(multi_haplo_test) >= int(3):
                    fh_tsv2.write('-'+'\t')
                    replaced_count+=1


            #And for single bp SNPS, just write as is
            else:
                fh_tsv2.write(haplo[index_choice]+'\t')
                ##print haplo[index_choice]
        if replaced_count >= int(1):
            fh_mia2.write("{}".format(line[0])+'\t'+"{}".format(replaced_count)+'\n')
            print '\t',"Locus {0}: recoded {1} polyhaplotypes".format(line_list[0],replaced_count)
        if replaced_count == int(line_list[1]):
            print '\n', '\t', "***WARNING: All samples recoded as missing data for locus {}!!!".format(line_list[0]),'\n'
                
        fh_tsv2.write('\n')
    fh_tsv2.close()
    print "Check output {} in {} directory.".format(tsv2_output,out_dir), '\n', '\n'
    fh_sum.write('\n'+"File '{}' written to {} directory.".format(tsv2_output,out_dir)+'\n')

#=================================================================================================
#execute subroutines in correct order with proper arguments based on user decision

#remove all loci with polyhaplotypes
if decision_analysis == "a":

    #get path to main dir to write output dir
    main_dir = sys.argv[2]
    os.chdir(main_dir)
    out_dir = "Summary_Output_A"
    if not os.path.exists(out_dir):
        os.mkdir(out_dir)
    os.chdir(out_dir)

    #create log file with summary info
    fh_sum = open("Summary.log", 'a')
    fh_sum.write("Summary for: {}".format(file_name)+'\n'+'\n')

    #subroutines
    loci_number = loci_number(hap_lines)
    invariant_list = identical_loci(hap_lines)
    multiple_haplos_list = poly_haplos(hap_lines)
    haplo_length(hap_lines)
    missing_data(hap_lines)
    flagged_list = biallelic(hap_lines)
    avoid_list = list_gathering(invariant_list,multiple_haplos_list,flagged_list,loci_number)
    output_tsv(hap_lines,avoid_list)
    output2_tsv(hap_lines,avoid_list)

    fh_sum.close()

#code samples with polyhaplotypes as missing data, rather than remove entire locus
elif decision_analysis == "b":

    #get path to main dir to write output dir
    main_dir = sys.argv[2]
    os.chdir(main_dir)
    out_dir = "Summary_Output_B"
    if not os.path.exists(out_dir):
        os.mkdir(out_dir)
    os.chdir(out_dir)

    #create log file with summary info
    fh_sum = open("Summary.log", 'a')
    fh_sum.write("Summary for: {}".format(file_name)+'\n'+'\n')

    loci_number = loci_number(hap_lines)
    invariant_list = identical_loci(hap_lines)
    multiple_haplos_list = poly_haplos(hap_lines)
    haplo_length(hap_lines)
    missing_data(hap_lines)
    flagged_list = biallelic(hap_lines)
    avoid_list_variation = list_gathering_variation(invariant_list,flagged_list,loci_number)
    output_tsv_variation(hap_lines,avoid_list_variation)
    output2_tsv_variation(hap_lines,avoid_list_variation)

    fh_sum.close()

    
elif decision_analysis == "q":
    print "Quitting program now", '\n'
else:
    print "Not a valid decision, now quitting", '\n'
